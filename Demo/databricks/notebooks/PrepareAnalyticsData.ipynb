{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3271a2-09c9-4611-a74d-05475db2758a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import col, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa71a975-fb0a-470f-8eba-7a1dd4e941a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadProcessedData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38a5942-de1c-4ee9-8255-1b1c6a4feb47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Data Lake paths (using the same approach as in InspectRawData)\n",
    "storage_account_name = \"datalakestoragetask\"  # Replace with your storage account name\n",
    "processed_container = \"processed\"\n",
    "analytics_ready_container = \"analytics-ready\"\n",
    "storage_key = \"\"  # Replace with your key or credential method\n",
    "\n",
    "# Configure Spark to access Azure Data Lake\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_key)\n",
    "\n",
    "# Define paths for processed and analytics-ready data\n",
    "paths = {\n",
    "    \"price_processed\": f\"abfss://{processed_container}@{storage_account_name}.dfs.core.windows.net/price_and_stock/price\",\n",
    "    \"stock_processed\": f\"abfss://{processed_container}@{storage_account_name}.dfs.core.windows.net/price_and_stock/stock\",\n",
    "    \"analytics_ready\": f\"abfss://{analytics_ready_container}@{storage_account_name}.dfs.core.windows.net\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4adb434-783d-4ba2-ad5b-26e394f2ab34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load DataFrames\n",
    "price_df = spark.read.format(\"parquet\").load(paths[\"price_processed\"])\n",
    "stock_df = spark.read.format(\"parquet\").load(paths[\"stock_processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6fc8fd-3a89-4db9-9b8d-3a89bfbe522b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Merge Data Using Composite Key (including manufacturer)\n",
    "merged_df = price_df.join(\n",
    "    stock_df,\n",
    "    on=[\"manufacturer\", \"manufacturer_pid\", \"retailer_pid\", \"order_unit\"],  # Updated Composite Key\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 3: Clean and Restructure Merged Data\n",
    "merged_df = merged_df.select(\n",
    "    col(\"manufacturer\"),  # Now a single column\n",
    "    col(\"price\").alias(\"price_amount\"),\n",
    "    col(\"price_base\").alias(\"price_base\"),\n",
    "    col(\"currency\").alias(\"currency\"),\n",
    "    col(\"tax_class\").alias(\"tax_class\"),\n",
    "    col(\"saleable\").alias(\"is_saleable\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\" replenishment_time\"),\n",
    "    col(\"deeplink\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59647a66-4e22-47f9-af8a-87a1423da843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Displaying Merged Data ---\n",
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- manufacturer_pid: string (nullable = true)\n",
      " |-- retailer_pid: string (nullable = true)\n",
      " |-- order_unit: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- price_base: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- tax_class: string (nullable = true)\n",
      " |-- saleable: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-513621778872262>, line 6\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m price_df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#merged_df.limit(30).show(truncate=False)\u001b[39;00m\n",
       "\u001b[0;32m----> 6\u001b[0m display(merged_df\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas())\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/display.py:152\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n",
       "\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.core.frame\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
       "\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n",
       "\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabricks.koalas.frame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyspark.pandas.frame\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n",
       "\u001b[1;32m    154\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
       "\u001b[1;32m    155\u001b[0m     index_col \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_col\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
       "\u001b[1;32m   1598\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n",
       "\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n",
       "\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n",
       "\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n",
       "\u001b[1;32m   1603\u001b[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001b[1;32m   1604\u001b[0m     )\n",
       "\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n",
       "\u001b[1;32m   1606\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
       "\u001b[1;32m   1607\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py:489\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
       "\u001b[1;32m    487\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\u001b[1;32m    488\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n",
       "\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
       "\u001b[1;32m   1660\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n",
       "\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[0;32m-> 1662\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
       "\u001b[1;32m   1663\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
       "\u001b[1;32m   1664\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n",
       "\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_createFromLocal\u001b[39m(\n",
       "\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001b[38;5;28mstr\u001b[39m]]]\n",
       "\u001b[1;32m   1223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple]\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType]:\n",
       "\u001b[1;32m   1224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001b[39;00m\n",
       "\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001b[39;00m\n",
       "\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m    write temp files.\u001b[39;00m\n",
       "\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
       "\u001b[0;32m-> 1229\u001b[0m     internal_data, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_data_schema(data, schema)\n",
       "\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1196\u001b[0m, in \u001b[0;36mSparkSession._wrap_data_schema\u001b[0;34m(self, data, schema)\u001b[0m\n",
       "\u001b[1;32m   1193\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
       "\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
       "\u001b[0;32m-> 1196\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
       "\u001b[1;32m   1197\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n",
       "\u001b[1;32m   1198\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n",
       "\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n",
       "\u001b[1;32m   1043\u001b[0m \n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n",
       "\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
       "\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m   1057\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n",
       "\u001b[1;32m   1058\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n",
       "\n",
       "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "can not infer schema from empty dataset"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: can not infer schema from empty dataset"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "File \u001b[0;32m<command-513621778872262>, line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m price_df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#merged_df.limit(30).show(truncate=False)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m display(merged_df\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas())\n",
        "File \u001b[0;32m/databricks/python_shell/dbruntime/display.py:152\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.core.frame\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabricks.koalas.frame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyspark.pandas.frame\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    155\u001b[0m     index_col \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_col\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1604\u001b[0m     )\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1606\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m )\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py:489\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    488\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1662\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1660\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1662\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1663\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m   1664\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1229\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_createFromLocal\u001b[39m(\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001b[38;5;28mstr\u001b[39m]]]\n\u001b[1;32m   1223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple]\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType]:\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m    write temp files.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1229\u001b[0m     internal_data, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_data_schema(data, schema)\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1196\u001b[0m, in \u001b[0;36mSparkSession._wrap_data_schema\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1196\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m   1197\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1198\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1056\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m   1058\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n",
        "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the merged DataFrame\n",
    "print(\"\\n--- Displaying Merged Data ---\")\n",
    "#merged_df.show(n=20, truncate=False)\n",
    "price_df.printSchema()\n",
    "merged_df.limit(30).show(truncate=False)\n",
    "display(merged_df.limit(10).toPandas())  # Display as table-like format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3e6e633-f41e-4b4e-a15d-78fe30e193fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Write Merged Data to Analytics-Ready Container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2726ec6-0775-4acb-be90-f2a477c06721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df.write.format(\"parquet\").mode(\"overwrite\").save(price_stock_analytics_path)\n",
    "\n",
    "print(\"\\n--- Merged Data Written to Analytics-Ready Container ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PrepareAnalyticsData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
