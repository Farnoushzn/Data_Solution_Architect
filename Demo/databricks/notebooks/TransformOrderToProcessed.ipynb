{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55901385-70cc-4ce7-9be7-522a4fba6459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, lit, split, concat, regexp_extract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b60e13-4ade-412b-96c5-9489e7e8f2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InspectRawData\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24dc6b07-bc78-4217-ae03-884bc4b75535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Data Lake paths (replace placeholders with actual values)\n",
    "storage_account_name = \"datalakestoragetask\"  # Replace with your storage account name\n",
    "raw_container = \"raw\"\n",
    "processed_container = \"processed\"\n",
    "storage_key = \"\"  # Replace with your key or credential method\n",
    "\n",
    "# Configure Spark to access Azure Data Lake\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_key)\n",
    "\n",
    "# Define paths for each dataset\n",
    "paths = {\n",
    "    \"order_raw\": f\"abfss://{raw_container}@{storage_account_name}.dfs.core.windows.net/order\",\n",
    "    \"order_processed\": f\"abfss://{processed_container}@{storage_account_name}.dfs.core.windows.net/order/\",\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4596f2-879b-4f6a-b85d-4b7aff6b1d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Inspection ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to inspect a dataset\n",
    "def inspect_dataset(name, path, format_type, options={}):\n",
    "    print(f\"\\n=== Inspecting {name} Dataset ===\")\n",
    "    try:\n",
    "        # Load dataset based on format\n",
    "        df = spark.read.format(format_type).options(**options).load(path)\n",
    "        \n",
    "        # Show schema and a sample of the data\n",
    "        df.printSchema()\n",
    "        df.limit(30).show(truncate=False)\n",
    "        \n",
    "        # Return DataFrame for further analysis if needed\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {name} data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Inspect datasets one by one\n",
    "print(\"\\n--- Starting Inspection ---\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b634ebda-ebe4-46a7-8636-6bec02aec3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Order Files ---\n",
      "Files in the Order folder:\n",
      "- example-order-1.edi (size: 252 bytes, modification time: 1733905923000)\n",
      "- example-order-2.edi (size: 261 bytes, modification time: 1733905923000)\n",
      "- example-order-3.edi (size: 293 bytes, modification time: 1733905923000)\n",
      "- example-order-4.edi (size: 249 bytes, modification time: 1733905923000)\n",
      "\n",
      "=== Inspecting Order Dataset ===\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|HDR;ORD;2.0;NOID;tequip;KDNR000001;Test.User;999999;;;2024-09-06T12:05;NORML;;;;;;;;test.user@nexmart.com;;false;test.user@nexmart.com;765412345;         |\n",
      "|ADR;SND;;;;;;;;                                                                                                                                           |\n",
      "|POS;0;5;;DEMO;90001;;2.0;;;;;;                                                                                                                            |\n",
      "|POS;0;5;;9999911111;9999911111;;50.0;;;;;;                                                                                                                |\n",
      "|QNT;SETU;PCE;                                                                                                                                             |\n",
      "|PRI;PCE;0.01;EUR;                                                                                                                                         |\n",
      "|PRI;SUM;0.52;EUR;                                                                                                                                         |\n",
      "|HDR;ORD;2.0;NOID;tequip;KDNR000001;Data.Example;999999;;;2024-09-06T12:05;NORML;;;;;;;;Data.Example@nexmart.com;;false;Data.Example@nexmart.com;765412345;|\n",
      "|ADR;SND;;;;;;;;                                                                                                                                           |\n",
      "|POS;0;5;;90001;90001;;14.0;;;;;;                                                                                                                          |\n",
      "|QNT;SETU;PCE;                                                                                                                                             |\n",
      "|PRI;PCE;1.11;EUR;                                                                                                                                         |\n",
      "|PRI;SUM;15.54;EUR;                                                                                                                                        |\n",
      "|HDR;ORD;2.0;NOID;tequip;KDNR000001;Test.User;999999;;;2024-09-06T12:05;NORML;;;;;;;;test.user@nexmart.com;;false;test.user@nexmart.com;765412345;         |\n",
      "|ADR;SND;;;;;;;;                                                                                                                                           |\n",
      "|POS;0;5;;90001;90001;;10.0;;;;;;                                                                                                                          |\n",
      "|QNT;SETU;PCE;                                                                                                                                             |\n",
      "|PRI;PCE;1.11;EUR;                                                                                                                                         |\n",
      "|PRI;SUM;11.10;EUR;                                                                                                                                        |\n",
      "|HDR;ORD;2.0;NOID;tequip;KDNR000001;Test.User;999999;;;2024-09-06T12:05;NORML;;;;;;;;test.user@nexmart.com;;false;test.user@nexmart.com;765412345;         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Log the files being processed\n",
    "print(\"\\n--- Processing Order Files ---\")\n",
    "order_files = dbutils.fs.ls(paths[\"order_raw\"])  # List all files in the folder\n",
    "\n",
    "# Log all file names\n",
    "print(\"Files in the Order folder:\")\n",
    "for file_info in order_files:\n",
    "    print(f\"- {file_info.name} (size: {file_info.size} bytes, modification time: {file_info.modificationTime})\")\n",
    "\n",
    "# Step 2: Read the Order dataset (combines all files in the folder)\n",
    "# Order data (EDI format - read as plain text)\n",
    "order_df = inspect_dataset(\n",
    "    name=\"Order\",\n",
    "    path=paths[\"order_raw\"],\n",
    "    format_type=\"text\"\n",
    ")\n",
    "#order_df = spark.read.text(paths[\"order_raw\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca6056b-843d-4f6a-acb8-08b354e47f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Order Files ---\n",
      "\n",
      "--- Checking for Completely Duplicate Files ---\n",
      "No duplicate files found.\n",
      "\n",
      "--- Unique Files to Be Processed ---\n",
      "- example-order-1.edi (size: 252 bytes)\n",
      "- example-order-2.edi (size: 261 bytes)\n",
      "- example-order-3.edi (size: 293 bytes)\n",
      "- example-order-4.edi (size: 249 bytes)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing Order Files ---\")\n",
    "order_files = dbutils.fs.ls(paths[\"order_raw\"])  # List all files in the folder\n",
    "\n",
    "# Step 1: Check for completely duplicate files\n",
    "print(\"\\n--- Checking for Completely Duplicate Files ---\")\n",
    "file_hashes = {}\n",
    "duplicate_files = []\n",
    "unique_files = []\n",
    "\n",
    "\n",
    "for file_info in order_files:\n",
    "    file_path = file_info.path\n",
    "    file_content = dbutils.fs.head(file_path)  # Read the first part of the file content\n",
    "    #file_content = dbutils.fs.head(file_path, 1024)  # Read the first 1024 bytes for hashing\n",
    "    file_hash = hash(file_content)  # Compute a hash for the file content\n",
    "\n",
    "    if file_hash in file_hashes:\n",
    "        duplicate_files.append(file_info.name)\n",
    "    else:\n",
    "        file_hashes[file_hash] = file_info.name\n",
    "        unique_files.append(file_path)\n",
    "\n",
    "# Log duplicate files (if any)\n",
    "if duplicate_files:\n",
    "    print(f\"Duplicate files found: {duplicate_files}\")\n",
    "else:\n",
    "    print(\"No duplicate files found.\")\n",
    "\n",
    "# Step 2: Filter and keep unique files\n",
    "#unique_files = [file_info for file_info in order_files if file_info.name not in duplicate_files]\n",
    "unique_files = [file_info.path for file_info in order_files if file_info.name not in duplicate_files]\n",
    "\n",
    "#order_df = spark.read.text(unique_files)\n",
    "\n",
    "# Log unique files being processed\n",
    "print(\"\\n--- Unique Files to Be Processed ---\")\n",
    "\n",
    "for file_info in order_files:\n",
    "    if file_info.path in unique_files:\n",
    "        print(f\"- {file_info.name} (size: {file_info.size} bytes)\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Step 3: Create a temporary path for unique files (if needed)\n",
    "# Here, we still use the folder path since all unique files are in the same folder\n",
    "#unique_path = paths[\"order_raw\"]  # Assuming unique files are left in the same folder after deduplication\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a93e6e-4693-4ad4-aef0-9e82a2560e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inspecting order Dataset ===\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>HDR;ORD;2.0;NOID;tequip;KDNR000001;Test.User;999999;;;2024-09-06T12:05;NORML;;;;;;;;test.user@nexmart.com;;false;test.user@nexmart.com;765412345;</td></tr><tr><td>ADR;SND;;;;;;;;</td></tr><tr><td>POS;0;5;;DEMO;90001;;2.0;;;;;;</td></tr><tr><td>POS;0;5;;9999911111;9999911111;;50.0;;;;;;</td></tr><tr><td>QNT;SETU;PCE;</td></tr><tr><td>PRI;PCE;0.01;EUR;</td></tr><tr><td>PRI;SUM;0.52;EUR;</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HDR;ORD;2.0;NOID;tequip;KDNR000001;Test.User;999999;;;2024-09-06T12:05;NORML;;;;;;;;test.user@nexmart.com;;false;test.user@nexmart.com;765412345;"
        ],
        [
         "ADR;SND;;;;;;;;"
        ],
        [
         "POS;0;5;;DEMO;90001;;2.0;;;;;;"
        ],
        [
         "POS;0;5;;9999911111;9999911111;;50.0;;;;;;"
        ],
        [
         "QNT;SETU;PCE;"
        ],
        [
         "PRI;PCE;0.01;EUR;"
        ],
        [
         "PRI;SUM;0.52;EUR;"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Inspecting order Dataset ===\")\n",
    "order_df.printSchema()\n",
    "display(order_df.limit(7).toPandas())  # Display as table-like format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ebd314-0bec-4d2d-903b-d329b867defd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+---------+------------+\n",
      "|order_id|value_PCE|currency_PCE|value_SUM|currency_SUM|\n",
      "+--------+---------+------------+---------+------------+\n",
      "|order_1 |1.11     |EUR         |NULL     |NULL        |\n",
      "|order_1 |NULL     |NULL        |11.10    |EUR         |\n",
      "|order_2 |1.11     |EUR         |NULL     |NULL        |\n",
      "|order_2 |NULL     |NULL        |15.54    |EUR         |\n",
      "|order_3 |0.01     |EUR         |NULL     |NULL        |\n",
      "|order_3 |NULL     |NULL        |0.52     |EUR         |\n",
      "|order_4 |0.01     |EUR         |NULL     |NULL        |\n",
      "|order_4 |NULL     |NULL        |0.14     |EUR         |\n",
      "+--------+---------+------------+---------+------------+\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = false)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- document_number: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- position_id: string (nullable = true)\n",
      " |-- line_number: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- alt_product_id: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- unit_of_measure: string (nullable = true)\n",
      " |-- quantity_type: string (nullable = true)\n",
      " |-- unit_of_measure: string (nullable = true)\n",
      " |-- value_PCE: string (nullable = true)\n",
      " |-- currency_PCE: string (nullable = true)\n",
      " |-- value_SUM: string (nullable = true)\n",
      " |-- currency_SUM: string (nullable = true)\n",
      "\n",
      "\n",
      "--- Order Processing Completed Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# Correcting the issue with file-wide order_id and proper interpretation of QNT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, first, lit, last, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "\n",
    "#edi_data = spark.read.text(unique_files)\n",
    "\n",
    "dataframes = []\n",
    "for idx, file_path in enumerate(unique_files):\n",
    "    edi_data = spark.read.text(file_path)\n",
    "    edi_data = edi_data.withColumn(\"order_id\", lit(f\"order_{idx + 1}\"))\n",
    "    dataframes.append(edi_data)\n",
    "\n",
    "# Combine all files into a single DataFrame\n",
    "data_with_order_id = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    data_with_order_id = data_with_order_id.union(df)\n",
    "\n",
    "# Step 1: Parse HDR Records (Header details)\n",
    "hdr_df = data_with_order_id.filter(col(\"value\").startswith(\"HDR\")).select(\n",
    "    col(\"order_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(1).alias(\"transaction_type\"),\n",
    "    split(col(\"value\"), \";\").getItem(2).alias(\"version\"),\n",
    "    split(col(\"value\"), \";\").getItem(3).alias(\"identifier\"),\n",
    "    split(col(\"value\"), \";\").getItem(4).alias(\"supplier\"),\n",
    "    split(col(\"value\"), \";\").getItem(5).alias(\"customer_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(6).alias(\"customer_name\"),\n",
    "    split(col(\"value\"), \";\").getItem(7).alias(\"document_number\"),\n",
    "    split(col(\"value\"), \";\").getItem(10).alias(\"order_date\"),\n",
    "    split(col(\"value\"), \";\").getItem(17).alias(\"email\")\n",
    ")\n",
    "\n",
    "# Step 2: Parse POS Records (Product details)\n",
    "pos_df = data_with_order_id.filter(col(\"value\").startswith(\"POS\")).select(\n",
    "    col(\"order_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(1).alias(\"position_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(2).alias(\"line_number\"),\n",
    "    split(col(\"value\"), \";\").getItem(4).alias(\"product_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(5).alias(\"alt_product_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(6).alias(\"quantity\"),\n",
    "    split(col(\"value\"), \";\").getItem(7).alias(\"unit_of_measure\")\n",
    ")\n",
    "\n",
    "# Step 3: Parse QNT Records (Quantity details)\n",
    "qnt_df = data_with_order_id.filter(col(\"value\").startswith(\"QNT\")).select(\n",
    "    col(\"order_id\"),\n",
    "    split(col(\"value\"), \";\").getItem(1).alias(\"quantity_type\"),\n",
    "    split(col(\"value\"), \";\").getItem(2).alias(\"unit_of_measure\")\n",
    ")\n",
    "\n",
    "# Step 4: Parse PRI Records (Pricing details)\n",
    "pri_df = data_with_order_id.filter(col(\"value\").startswith(\"PRI\")).select(\n",
    "    col(\"order_id\"),\n",
    "    when(split(col(\"value\"), \";\").getItem(1) == \"PCE\", split(col(\"value\"), \";\").getItem(2)).alias(\"value_PCE\"),\n",
    "    when(split(col(\"value\"), \";\").getItem(1) == \"PCE\", split(col(\"value\"), \";\").getItem(3)).alias(\"currency_PCE\"),\n",
    "    when(split(col(\"value\"), \";\").getItem(1) == \"SUM\", split(col(\"value\"), \";\").getItem(2)).alias(\"value_SUM\"),\n",
    "    when(split(col(\"value\"), \";\").getItem(1) == \"SUM\", split(col(\"value\"), \";\").getItem(3)).alias(\"currency_SUM\")\n",
    ")\n",
    "\n",
    "# Verify if SUM lines are correctly parsed\n",
    "pri_df.show(truncate=False)\n",
    "\n",
    "# Consolidate PRI data\n",
    "pri_consolidated_df = pri_df.groupBy(\"order_id\").agg(\n",
    "    first(\"value_PCE\", ignorenulls=True).alias(\"value_PCE\"),\n",
    "    first(\"currency_PCE\", ignorenulls=True).alias(\"currency_PCE\"),\n",
    "    first(\"value_SUM\", ignorenulls=True).alias(\"value_SUM\"),\n",
    "    first(\"currency_SUM\", ignorenulls=True).alias(\"currency_SUM\")\n",
    ")\n",
    "\n",
    "# Combine HDR, POS, QNT, and PRI data into a unified dataset\n",
    "order_transformed = hdr_df.join(pos_df, \"order_id\", \"left\")\n",
    "order_transformed = order_transformed.join(qnt_df, \"order_id\", \"left\")\n",
    "order_transformed = order_transformed.join(pri_consolidated_df, \"order_id\", \"left\")\n",
    "\n",
    "\n",
    "# Show the schema and data to verify the correctness\n",
    "order_transformed.printSchema()\n",
    "print(\"\\n--- Order Processing Completed Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68f1485-ba90-4d3b-ac98-8542675501f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = false)\n",
      " |-- value_PCE: string (nullable = true)\n",
      " |-- currency_PCE: string (nullable = true)\n",
      " |-- value_SUM: string (nullable = true)\n",
      " |-- currency_SUM: string (nullable = true)\n",
      "\n",
      "+--------+---------+------------+---------+------------+\n",
      "|order_id|value_PCE|currency_PCE|value_SUM|currency_SUM|\n",
      "+--------+---------+------------+---------+------------+\n",
      "|order_1 |1.11     |EUR         |11.10    |EUR         |\n",
      "|order_2 |1.11     |EUR         |15.54    |EUR         |\n",
      "|order_3 |0.01     |EUR         |0.52     |EUR         |\n",
      "|order_4 |0.01     |EUR         |0.14     |EUR         |\n",
      "+--------+---------+------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pri_consolidated_df.printSchema()\n",
    "pri_consolidated_df.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb9afea-4a76-4d6b-a6b1-cbb6ee0f0d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inspecting order Dataset ===\n",
      "root\n",
      " |-- order_id: string (nullable = false)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- document_number: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- position_id: string (nullable = true)\n",
      " |-- line_number: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- alt_product_id: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- unit_of_measure: string (nullable = true)\n",
      " |-- quantity_type: string (nullable = true)\n",
      " |-- unit_of_measure: string (nullable = true)\n",
      " |-- value_PCE: string (nullable = true)\n",
      " |-- currency_PCE: string (nullable = true)\n",
      " |-- value_SUM: string (nullable = true)\n",
      " |-- currency_SUM: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:477: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Duplicate column names found: ['order_id', 'transaction_type', 'version', 'identifier', 'supplier', 'customer_id', 'customer_name', 'document_number', 'order_date', 'email', 'position_id', 'line_number', 'product_id', 'alt_product_id', 'quantity', 'unit_of_measure', 'quantity_type', 'unit_of_measure', 'value_PCE', 'currency_PCE', 'value_SUM', 'currency_SUM']\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>transaction_type</th><th>version</th><th>identifier</th><th>supplier</th><th>customer_id</th><th>customer_name</th><th>document_number</th><th>order_date</th><th>email</th><th>position_id</th><th>line_number</th><th>product_id</th><th>alt_product_id</th><th>quantity</th><th>unit_of_measure</th><th>quantity_type</th><th>unit_of_measure</th><th>value_PCE</th><th>currency_PCE</th><th>value_SUM</th><th>currency_SUM</th></tr></thead><tbody><tr><td>order_1</td><td>ORD</td><td>2.0</td><td>NOID</td><td>tequip</td><td>KDNR000001</td><td>Test.User</td><td>999999</td><td>2024-09-06T12:05</td><td></td><td>0</td><td>5</td><td>90001</td><td>90001</td><td></td><td>10.0</td><td>SETU</td><td>PCE</td><td>1.11</td><td>EUR</td><td>11.10</td><td>EUR</td></tr><tr><td>order_2</td><td>ORD</td><td>2.0</td><td>NOID</td><td>tequip</td><td>KDNR000001</td><td>Data.Example</td><td>999999</td><td>2024-09-06T12:05</td><td></td><td>0</td><td>5</td><td>90001</td><td>90001</td><td></td><td>14.0</td><td>SETU</td><td>PCE</td><td>1.11</td><td>EUR</td><td>15.54</td><td>EUR</td></tr><tr><td>order_3</td><td>ORD</td><td>2.0</td><td>NOID</td><td>tequip</td><td>KDNR000001</td><td>Test.User</td><td>999999</td><td>2024-09-06T12:05</td><td></td><td>0</td><td>5</td><td>9999911111</td><td>9999911111</td><td></td><td>50.0</td><td>SETU</td><td>PCE</td><td>0.01</td><td>EUR</td><td>0.52</td><td>EUR</td></tr><tr><td>order_3</td><td>ORD</td><td>2.0</td><td>NOID</td><td>tequip</td><td>KDNR000001</td><td>Test.User</td><td>999999</td><td>2024-09-06T12:05</td><td></td><td>0</td><td>5</td><td>DEMO</td><td>90001</td><td></td><td>2.0</td><td>SETU</td><td>PCE</td><td>0.01</td><td>EUR</td><td>0.52</td><td>EUR</td></tr><tr><td>order_4</td><td>ORD</td><td>2.0</td><td>NOID</td><td>tequip</td><td>KDNR000001</td><td>Test.User</td><td>999999</td><td>2024-09-06T12:05</td><td></td><td>0</td><td>5</td><td>DEMO</td><td>DEMO</td><td></td><td>14.0</td><td>SETU</td><td>PCE</td><td>0.01</td><td>EUR</td><td>0.14</td><td>EUR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "order_1",
         "ORD",
         "2.0",
         "NOID",
         "tequip",
         "KDNR000001",
         "Test.User",
         "999999",
         "2024-09-06T12:05",
         "",
         "0",
         "5",
         "90001",
         "90001",
         "",
         "10.0",
         "SETU",
         "PCE",
         "1.11",
         "EUR",
         "11.10",
         "EUR"
        ],
        [
         "order_2",
         "ORD",
         "2.0",
         "NOID",
         "tequip",
         "KDNR000001",
         "Data.Example",
         "999999",
         "2024-09-06T12:05",
         "",
         "0",
         "5",
         "90001",
         "90001",
         "",
         "14.0",
         "SETU",
         "PCE",
         "1.11",
         "EUR",
         "15.54",
         "EUR"
        ],
        [
         "order_3",
         "ORD",
         "2.0",
         "NOID",
         "tequip",
         "KDNR000001",
         "Test.User",
         "999999",
         "2024-09-06T12:05",
         "",
         "0",
         "5",
         "9999911111",
         "9999911111",
         "",
         "50.0",
         "SETU",
         "PCE",
         "0.01",
         "EUR",
         "0.52",
         "EUR"
        ],
        [
         "order_3",
         "ORD",
         "2.0",
         "NOID",
         "tequip",
         "KDNR000001",
         "Test.User",
         "999999",
         "2024-09-06T12:05",
         "",
         "0",
         "5",
         "DEMO",
         "90001",
         "",
         "2.0",
         "SETU",
         "PCE",
         "0.01",
         "EUR",
         "0.52",
         "EUR"
        ],
        [
         "order_4",
         "ORD",
         "2.0",
         "NOID",
         "tequip",
         "KDNR000001",
         "Test.User",
         "999999",
         "2024-09-06T12:05",
         "",
         "0",
         "5",
         "DEMO",
         "DEMO",
         "",
         "14.0",
         "SETU",
         "PCE",
         "0.01",
         "EUR",
         "0.14",
         "EUR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "identifier",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "supplier",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "document_number",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "position_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "line_number",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "alt_product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "unit_of_measure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "unit_of_measure",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value_PCE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "currency_PCE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value_SUM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "currency_SUM",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Inspecting order Dataset ===\")\n",
    "order_transformed.printSchema()\n",
    "display(order_transformed.limit(10).toPandas())  # Display as table-like format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba8a526-cbdd-4d41-b3f5-6d4a7167c197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Write Transformed Data to Processed Folder\n",
    "Save the transformed datasets into the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfc883c-ea8c-4c9f-8a01-d727ef45f111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_transformed.write.format(\"parquet\").mode(\"overwrite\").save(paths[\"order_processed\"])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TransformOrderToProcessed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
